name_model = "150M"
project = "150M_big_bs"
type_model = "llama2"

[train]
micro_bs = 64 # change this base on the gpu
reshard_after_forward = false

[optim]
batch_size = 2048
warmup_steps = 500
total_steps = 4096


[optim.optim]
lr = 8e-4
