model = "PrimeIntellect/Qwen3-0.6B-Reverse-Text-SFT"
max_steps = 1000

# env_file = ["secrets.env"] # optional file(s) for keys/secrets

# Training
batch_size = 128
rollouts_per_example = 8
# trajectory_strategy = "interleaved"  # or "branching"
# learning_rate = 1e-6
# lora_alpha = 16
# oversampling_factor = 1.0
max_async_level = 4

[sampling]
max_tokens = 2048
# temperature = 0.7

[[env]]
id = "d42me/reverse-text"

# [[env]] # add multiple [[env]] sections for multi-env training
# id = "primeintellect/another-env"
# args = { split = "train", max_examples = 1000 }

# Optional: W&B logging
# [wandb]
# project = "my-project"
# entity = "my-team"
# name = "my-run-name"

# Optional: online evaluation
# [eval]
# interval = 100
# # optional: default for all environments
# num_examples = -1
# rollouts_per_example = 1
# eval_base_model = true
#
# [[eval.env]]
# id = "primeintellect/eval-env"
# args = { split = "test" }
# # environment-specific overrides
# num_examples = 30
# rollouts_per_example = 4

# Optional: validation during training
# [val]
# num_examples = 64
# rollouts_per_example = 1
# interval = 5

# Optional: buffer configuration for difficulty filtering
# [buffer]
# easy_threshold = 0.8
# hard_threshold = 0.2
# easy_fraction = 0.0
# hard_fraction = 0.0
# online_difficulty_filtering = false
# env_ratios = [0.5, 0.5]
# skip_verification = false
# seed = 42
